defaults:
    - _self_
    - rl: ac_tdmpc2
    - env/jsbsim: noatmo
    - env/task/mdp: no_hist_all
    - env/task/reward: actvar
    - override hydra/launcher: submitit_local

# training
# reference attitude angle limits (+/- degrees)
roll_limit: 60 # 45 up to moderate, 60 up to hard
pitch_limit: 30 # 25 up to moderate, 30 up to hard

rl:
    # environment
    task: dog-run
    obs: state

    # evaluation
    checkpoint: ???
    periodic_eval: true
    eval_episodes: 10
    eval_freq: 50000

    # training
    steps: 10_000_000
    batch_size: 256
    reward_coef: 0.1
    value_coef: 0.1
    consistency_coef: 20
    decoder_coef: 1
    rho: 0.5
    lr: 3e-4
    enc_lr_scale: 0.3
    grad_clip_norm: 20
    tau: 0.01
    discount: ??? # if unspecified, computed from heuristic
    discount_denom: 5
    discount_min: 0.95
    discount_max: 0.995
    buffer_size: 1_000_000
    exp_name: default
    data_dir: ???
    final_traj: ???

    # CAPS
    use_caps: false
    ts_coef: 0.0

    # planning
    mpc: true
    iterations: 6
    num_samples: 512
    num_elites: 64
    num_pi_trajs: 24
    horizon: 3
    min_std: 0.05
    max_std: 2
    temperature: 0.5

    # actor
    log_std_min: -10
    log_std_max: 2
    entropy_coef: 1e-4

    # critic
    num_bins: 101
    vmin: -10
    vmax: +10

    # architecture
    model_size: ???
    num_enc_layers: 2
    enc_dim: 256
    latent_dim: 512
    num_channels: 32
    mlp_dim: 512
    task_dim: 96
    num_q: 5
    dropout: 0.01
    simnorm_dim: 8

    # logging
    wandb_project: ???
    wandb_entity: ???
    wandb_silent: false
    disable_wandb: true
    save_csv: true

    # misc
    save_video: false
    save_agent: false
    seed: ???

    # convenience
    work_dir: ???
    task_title: ???
    multitask: ???
    tasks: ???
    obs_shape: ???
    action_dim: ???
    episode_length: ???
    obs_shapes: ???
    action_dims: ???
    episode_lengths: ???
    seed_steps: ???
    bin_size: ???
