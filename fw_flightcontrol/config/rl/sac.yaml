SAC:
    exp_name: default
    seed: ???
    torch_deterministic: True
    cuda: True
    track: True
    wandb_project_name: uav_rl
    wandb_entity: thesedavid
    capture_video: False

    # Algorithm specific arguments
    env_id: ACBohnNoVaIErr-v0
    total_timesteps: 750_000 # total timesteps to train the agent
    buffer_size: 1e6 # replay buffer size
    gamma: 0.99 # discount factor
    tau: 0.005 # target smoothing coefficient
    batch_size: 256 # the batch size of sample from the replay memory
    learning_starts: 5e3 # timestep to start learning
    policy_lr: 3e-4 # the learning rate of the policy network optimizer
    q_lr: 1e-3 # the learning rate of the Q network network optimizer
    policy_frequency: 2 # the frequency of training policy (delayed)
    target_network_frequency: 1  # The frequency of updates for the target networks. Denis Yarats' implementation delays this by 2.
    noise_clip: 0.5 # noise clip parameter of the Target Policy Smoothing Regularization
    alpha: 0.2 # Entropy regularization coefficient.
    autotune: True # automatic tuning of the entropy coefficient

    # periodic evaluation during training
    periodic_eval: True
    eval_freq: 37_500

    final_traj_plot: True # uploading a test traj telemetry of the agent at the end of training, in wandb

    # CAPS loss
    ts_coef: 0.05
